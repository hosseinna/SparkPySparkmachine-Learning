{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql  import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator#to measure the performance of the  ALS model \n",
    "from pyspark.ml.tuning  import TrainValidationSplit , ParamGridBuilder#allow us to cross validate and fine-tune  the hyper parameter of the model\n",
    "from pyspark.ml.recommendation import ALS\n",
    "#create session\n",
    "appName = \"Recommender system in Spark\"\n",
    "spark = SparkSession.builder.appName(appName).config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file into dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file into dataFrame using automatically inferred schema\n",
    "ratings = spark.read.csv('Path/ratings.csv', inferSchema=True, header=True ,nanValue=\"NA\")\n",
    "movies = spark.read.csv('Path/movies.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "+------+-------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------------+\n",
      "|movieId|           title|              genres|\n",
      "+-------+----------------+--------------------+\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|  Jumanji (1995)|Adventure|Childre...|\n",
      "+-------+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#merge \"movies\" and \"ratings\" dataFrame based on \"movieId\"\n",
    "df=ratings.join(movies, \"movieId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|               title|              genres|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|     31|     1|   2.5|1260759144|Dangerous Minds (...|               Drama|\n",
      "|   1029|     1|   3.0|1260759179|        Dumbo (1941)|Animation|Childre...|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data rows: 70047 , number of testing data rows: 29957\n"
     ]
    }
   ],
   "source": [
    "#use only column data of \"userId\", \"movieId\", dan \"rating\"\n",
    "data = df.select(\"userId\", \"movieId\", \"rating\")\n",
    "#divide data, 70% for training and 30% for testing\n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0].withColumnRenamed(\"rating\", \"label\")\n",
    "test = splits[1].withColumnRenamed(\"rating\", \"trueLabel\")\n",
    "#calculate number of rows\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print (\"number of training data rows:\", train_rows, \n",
    "       \", number of testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "#define ALS (Alternating Least Square) as our recommender system\n",
    "als = ALS(maxIter=19, regParam=0.01,rank=20, userCol=\"userId\", \n",
    "          itemCol=\"movieId\", ratingCol=\"label\")\n",
    "#train our ALS model\n",
    "model = als.fit(train)\n",
    "print(\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing is done!\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "print(\"testing is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+----------+---------+\n",
      "|userId|title                      |prediction|trueLabel|\n",
      "+------+---------------------------+----------+---------+\n",
      "|232   |Guilty as Sin (1993)       |1.7766558 |4.0      |\n",
      "|285   |Hudsucker Proxy, The (1994)|4.5955706 |5.0      |\n",
      "|491   |Hudsucker Proxy, The (1994)|4.1515136 |3.0      |\n",
      "|299   |Hudsucker Proxy, The (1994)|3.1894815 |4.5      |\n",
      "|309   |Hudsucker Proxy, The (1994)|3.4492915 |4.0      |\n",
      "+------+---------------------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.join(movies, \"movieId\").select(\n",
    "    \"userId\", \"title\", \"prediction\", \"trueLabel\").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRecommendets=model.recommendForAllUsers(5)\n",
    "moviereommends=model.recommendForAllItems(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = false)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- movieId: integer (nullable = true)\n",
      " |    |    |-- rating: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecommendets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------+\n",
      "|userId|movieId                       |\n",
      "+------+------------------------------+\n",
      "|471   |[1997, 2712, 3578, 1259, 3039]|\n",
      "|463   |[1272, 260, 4128, 2973, 59315]|\n",
      "|496   |[1304, 4993, 5952, 3095, 260] |\n",
      "|148   |[953, 1233, 17, 1077, 151]    |\n",
      "|540   |[8376, 1883, 5995, 8665, 1088]|\n",
      "+------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecommendets.select(\"userId\" , \"recommendations.movieId\").show(5 , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = false)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- userId: integer (nullable = true)\n",
      " |    |    |-- rating: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviereommends.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------+\n",
      "|movieId|userId                   |\n",
      "+-------+-------------------------+\n",
      "|1580   |[145, 134, 46, 136, 202] |\n",
      "|5300   |[257, 604, 151, 657, 348]|\n",
      "|6620   |[6, 614, 59, 304, 545]   |\n",
      "|7340   |[621, 448, 155, 535, 232]|\n",
      "|32460  |[298, 365, 197, 53, 568] |\n",
      "+-------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviereommends.select(\"movieId\" , \"recommendations.userId\").show(5 , False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): nan\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original data rows:  29957\n",
      "number of rows after dropping data with missing value:  28769\n",
      "number of missing data:  1188\n"
     ]
    }
   ],
   "source": [
    "prediction.count()\n",
    "a = prediction.count()\n",
    "print(\"number of original data rows: \", a)\n",
    "#drop rows with any missing data\n",
    "cleanPred = prediction.dropna(how=\"any\", subset=[\"prediction\"])\n",
    "b = cleanPred.count()\n",
    "print(\"number of rows after dropping data with missing value: \", b)\n",
    "print(\"number of missing data: \", a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 1.3414486717928011\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(cleanPred)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder().addGrid(als.rank, [5, 40, 80, 120]).addGrid(als.maxIter, [5, 100, 250, 500]).addGrid(als.regParam, [.05, .1, 1.5]).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+---------------------------------+\n",
      "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
      "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
      "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
      "|1477  |0        |Sentiment140   |this game is awful #nightmare    |\n",
      "|45    |1        |Sentiment140   |I love jam #loveit               |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read data file into Spark dataFrame\n",
    "#read csv file into dataFrame with automatically inferred schema\n",
    "tweets_csv = spark.read.csv('Path/tweets.csv', inferSchema=True, header=True)\n",
    "tweets_csv.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "|this game is awful #nightmare    |0    |\n",
      "|I love jam #loveit               |1    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#select only \"SentimentText\" and \"Sentiment\" column, \n",
    "#and cast \"Sentiment\" column data into integer\n",
    "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data.show(truncate = False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1364 ; Testing data rows: 568\n"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Separate \"SentimentText\" into individual words using tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+---------------------------------------+\n",
      "|SentimentText                    |label|SentimentWords                         |\n",
      "+---------------------------------+-----+---------------------------------------+\n",
      "|I adore cheese #bestever         |1    |[i, adore, cheese, #bestever]          |\n",
      "|I adore cheese #brilliant        |1    |[i, adore, cheese, #brilliant]         |\n",
      "|I adore cheese #loveit           |1    |[i, adore, cheese, #loveit]            |\n",
      "|I adore cheese #toptastic        |1    |[i, adore, cheese, #toptastic]         |\n",
      "|I adore classical music #bestever|1    |[i, adore, classical, music, #bestever]|\n",
      "+---------------------------------+-----+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words (unimportant words to be features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
      "|SentimentText                     |label|SentimentWords                          |MeaningfulWords                      |\n",
      "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
      "|I adore cheese #bestever          |1    |[i, adore, cheese, #bestever]           |[adore, cheese, #bestever]           |\n",
      "|I adore cheese #brilliant         |1    |[i, adore, cheese, #brilliant]          |[adore, cheese, #brilliant]          |\n",
      "|I adore cheese #loveit            |1    |[i, adore, cheese, #loveit]             |[adore, cheese, #loveit]             |\n",
      "|I adore cheese #toptastic         |1    |[i, adore, cheese, #toptastic]          |[adore, cheese, #toptastic]          |\n",
      "|I adore classical music #bestever |1    |[i, adore, classical, music, #bestever] |[adore, classical, music, #bestever] |\n",
      "|I adore classical music #brilliant|1    |[i, adore, classical, music, #brilliant]|[adore, classical, music, #brilliant]|\n",
      "|I adore classical music #favorite |1    |[i, adore, classical, music, #favorite] |[adore, classical, music, #favorite] |\n",
      "|I adore classical music #loveit   |1    |[i, adore, classical, music, #loveit]   |[adore, classical, music, #loveit]   |\n",
      "|I adore classical music #thumbs-up|1    |[i, adore, classical, music, #thumbs-up]|[adore, classical, music, #thumbs-up]|\n",
      "|I adore classical music #toptastic|1    |[i, adore, classical, music, #toptastic]|[adore, classical, music, #toptastic]|\n",
      "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting words feature into numerical feature. In Spark 3.0.0,it is implemented in HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+-------------------------------------------+\n",
      "|label|MeaningfulWords            |features                                   |\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #bestever] |(262144,[1689,91011,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #brilliant]|(262144,[1689,45361,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #loveit]   |(262144,[1689,100089,254974],[1.0,1.0,1.0])|\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'label', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our classifier model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+-------------------------------------------+\n",
      "|Label|MeaningfulWords            |features                                   |\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #favorite] |(262144,[1689,100089,108624],[1.0,1.0,1.0])|\n",
      "|1    |[adore, cheese, #thumbs-up]|(262144,[1689,88825,100089],[1.0,1.0,1.0]) |\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict testing data and calculate the accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+-----+\n",
      "|MeaningfulWords                |prediction|Label|\n",
      "+-------------------------------+----------+-----+\n",
      "|[adore, cheese, #favorite]     |1.0       |1    |\n",
      "|[adore, cheese, #thumbs-up]    |1.0       |1    |\n",
      "|[adore, coffee, #favorite]     |1.0       |1    |\n",
      "|[adore, pop, music, #toptastic]|1.0       |1    |\n",
      "|[adore, rock, music, #favorite]|1.0       |1    |\n",
      "+-------------------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "correct prediction: 556 , total data: 568 , accuracy: 0.9788732394366197\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=5, truncate = False)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
