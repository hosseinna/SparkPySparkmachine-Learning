{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Import modules and create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "\n",
    "#create Spark session\n",
    "appName = \"Regression in Spark\"\n",
    "spark = SparkSession.builder.appName(appName).config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data file into Spark dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flightSchema = StructType([\n",
    "  StructField(\"DayofMonth\", IntegerType(), False),\n",
    "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
    "  StructField(\"Carrier\", StringType(), False),\n",
    "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
    "  StructField(\"DestAirportID\", IntegerType(), False),\n",
    "  StructField(\"DepDelay\", IntegerType(), False),\n",
    "  StructField(\"ArrDelay\", IntegerType(), False),\n",
    "])\n",
    "#read csv data with our defined schema\n",
    "flightDataFrame = spark.read.csv('C:/Users/hossein/Downloads/dataset/flights.csv',schema=flightSchema,header=True)\n",
    "flightDataFrame.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select important data for regression feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|        19|        5|          11433|        13303|      -3|       1|\n",
      "|        19|        5|          14869|        12478|       0|      -8|\n",
      "|        19|        5|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select related column data for our regression input features\n",
    "data = flightDataFrame.select(\"DayofMonth\", \"DayOfWeek\", \n",
    "                              \"OriginAirportID\", \"DestAirportID\", \n",
    "                              \"DepDelay\", \"ArrDelay\")\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2702218"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 2161910 ; Testing data rows: 540308\n"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.8, 0.2] ) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|         1|        1|          10140|        10397|      -4|     -11|\n",
      "|         1|        1|          10140|        10397|      -2|     -17|\n",
      "|         1|        1|          10140|        10821|       8|      -9|\n",
      "|         1|        1|          10140|        11259|      -2|     -14|\n",
      "|         1|        1|          10140|        11259|       0|     -12|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|-11  |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|-17  |\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |-9   |\n",
      "+------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define an assembler\n",
    "assembler = VectorAssembler(inputCols = [\n",
    "    \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "    \"DepDelay\"], outputCol=\"features\")\n",
    "#change our features into one column using our defined assembler\n",
    "trainingDataFinal = assembler.transform(trainingData).select(\n",
    "    col(\"features\"), (col(\"ArrDelay\").cast(\"Int\").alias(\"label\")))\n",
    "trainingDataFinal.show(truncate=False , n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our regression model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression model is trained!\n"
     ]
    }
   ],
   "source": [
    "#call Spark linear regression we import before\n",
    "algoritma = LinearRegression(\n",
    "    labelCol=\"label\",featuresCol=\"features\", \n",
    "    maxIter=10, regParam=0.3)\n",
    "#train the model\n",
    "model = algoritma.fit(trainingDataFinal)\n",
    "print (\"Regression model is trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------+\n",
      "|features                      |trueLabel|\n",
      "+------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,11259.0,-3.0]|-11      |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|-11      |\n",
      "+------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change our feature data into one column using our defined assembler\n",
    "#just like what we did before in the training data\n",
    "testingDataFinal = assembler.transform(\n",
    "    testingData).select(\n",
    "    col(\"features\"), (col(\"ArrDelay\")).cast(\"Int\").alias(\"trueLabel\"))\n",
    "testingDataFinal.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the testing data using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------------+\n",
      "|            features|trueLabel|         prediction|\n",
      "+--------------------+---------+-------------------+\n",
      "|[1.0,1.0,10140.0,...|      -11| -6.741786539729593|\n",
      "|[1.0,1.0,10140.0,...|      -11| -4.747268178110664|\n",
      "|[1.0,1.0,10140.0,...|       41|  31.15406233103006|\n",
      "|[1.0,1.0,10140.0,...|       -8| -7.746704913558036|\n",
      "|[1.0,1.0,10140.0,...|      -13|-13.731652578963729|\n",
      "|[1.0,1.0,10140.0,...|       -2| -5.753579132488012|\n",
      "|[1.0,1.0,10140.0,...|       18| 27.945970610005077|\n",
      "|[1.0,1.0,10140.0,...|      -16| -8.970026336806427|\n",
      "|[1.0,1.0,10140.0,...|      812|  831.7194630855722|\n",
      "|[1.0,1.0,10140.0,...|       -9| -6.122845074706012|\n",
      "+--------------------+---------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict testing data using our model\n",
    "prediction = model.transform(testingDataFinal)\n",
    "#show some prediction results\n",
    "prediction.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 13.196052557836095\n"
     ]
    }
   ],
   "source": [
    "#import evaluator module for regression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#define our evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "#calculate RMSE of our trained model\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification-LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select important data for classification features and change arrival delay into binary class \"late\" vs \"not late\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('C:/Users/hossein/Downloads/dataset/flights.csv',schema=flightSchema,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-------------+--------+----+\n",
      "|DayofMonth|DayOfWeek|OriginAirportID|DestAirportID|DepDelay|Late|\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "|        19|        5|          11433|        13303|      -3|   0|\n",
      "|        19|        5|          14869|        12478|       0|   0|\n",
      "|        19|        5|          14057|        14869|      -4|   0|\n",
      "|        19|        5|          15016|        11433|      28|   1|\n",
      "|        19|        5|          11193|        12892|      -6|   0|\n",
      "|        19|        5|          10397|        15016|      -1|   0|\n",
      "|        19|        5|          15016|        10397|       0|   0|\n",
      "|        19|        5|          10397|        14869|      15|   1|\n",
      "|        19|        5|          10397|        10423|      33|   1|\n",
      "|        19|        5|          11278|        10397|     323|   1|\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data =df.select(\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "\"DepDelay\", ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"Late\")))\n",
    "\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 2161292 ; Testing data rows: 540926\n"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.8, 0.2],seed=10) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0    |\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |0    |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|0    |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|0    |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0] |0    |\n",
      "+------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "    \"DepDelay\"], outputCol=\"features\")\n",
    "trainingDataFinal = assembler.transform(\n",
    "    trainingData).select(col(\"features\"), col(\"Late\").alias(\"label\"))\n",
    "trainingDataFinal.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our classifier model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier model is trained!\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(\n",
    "    labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
    "#train our classifier\n",
    "model = classifier.fit(trainingDataFinal)\n",
    "print (\"Classifier model is trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0005653235784771402,-0.0039562389945402955,-1.1156984891306539e-05,-2.863345324179474e-06,0.013976316968702994]\n",
      "Intercept: -1.392177156557798\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "mlrModel = mlr.fit(trainingDataFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial coefficients: 2 X 5 CSRMatrix\n",
      "(0,4) -0.0001\n",
      "(1,4) 0.0001\n",
      "Multinomial intercepts: [0.6964283915935823,-0.6964283915935823]\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|1.857829590580804E-5|\n",
      "|0.0|3.715659181161608E-5|\n",
      "|0.0|5.805717470565012...|\n",
      "|0.0|7.895775759968417E-5|\n",
      "|0.0|1.021806274819442...|\n",
      "|0.0|1.254034973642042...|\n",
      "|0.0|1.463040802582383E-4|\n",
      "|0.0|1.648823761640463...|\n",
      "|0.0|1.834606720698544E-4|\n",
      "|0.0|2.066835419521144...|\n",
      "|0.0|2.275841248461485E-4|\n",
      "|0.0|2.554515687048605...|\n",
      "|0.0|2.786744385871206E-4|\n",
      "|0.0|3.018973084693806...|\n",
      "|0.0|3.135087434105106...|\n",
      "|0.0|3.413761872692227E-4|\n",
      "|0.0|3.622767701632567...|\n",
      "|0.0|3.878219270337428...|\n",
      "|0.0|4.156893708924549E-4|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = mlrModel.summary\n",
    "trainingSummary.roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|1.857829590580804E-5|\n",
      "|0.0|3.715659181161608E-5|\n",
      "|0.0|5.805717470565012...|\n",
      "|0.0|7.895775759968417E-5|\n",
      "|0.0|1.021806274819442...|\n",
      "|0.0|1.254034973642042...|\n",
      "|0.0|1.463040802582383E-4|\n",
      "|0.0|1.648823761640463...|\n",
      "|0.0|1.834606720698544E-4|\n",
      "|0.0|2.066835419521144...|\n",
      "|0.0|2.275841248461485E-4|\n",
      "|0.0|2.554515687048605...|\n",
      "|0.0|2.786744385871206E-4|\n",
      "|0.0|3.018973084693806...|\n",
      "|0.0|3.135087434105106...|\n",
      "|0.0|3.413761872692227E-4|\n",
      "|0.0|3.622767701632567...|\n",
      "|0.0|3.878219270337428...|\n",
      "|0.0|4.156893708924549E-4|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 0.9182793015801936\n"
     ]
    }
   ],
   "source": [
    " #Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|trueLabel|\n",
      "+--------------------+---------+\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testingDataFinal = assembler.transform(\n",
    "    testingData).select(col(\"features\"), col(\"Late\").alias(\"trueLabel\"))\n",
    "testingDataFinal.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the testing data using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+----------------------------------------+---------+\n",
      "|features                       |prediction|probability                             |trueLabel|\n",
      "+-------------------------------+----------+----------------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0] |0.0       |[0.8272730425926873,0.1727269574073127] |0        |\n",
      "|[1.0,1.0,10140.0,11259.0,-3.0] |0.0       |[0.8296102079456423,0.17038979205435775]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0]  |0.0       |[0.8236010855806822,0.17639891441931776]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,35.0] |0.0       |[0.7411141075624572,0.25888589243754273]|1        |\n",
      "|[1.0,1.0,10140.0,11292.0,-1.0] |0.0       |[0.8256360260370816,0.17436397396291847]|0        |\n",
      "|[1.0,1.0,10140.0,11292.0,4.0]  |0.0       |[0.8153458143542055,0.18465418564579456]|0        |\n",
      "|[1.0,1.0,10140.0,12266.0,838.0]|1.0       |[3.83647958813945E-5,0.9999616352041186]|1        |\n",
      "|[1.0,1.0,10140.0,12889.0,-2.0] |0.0       |[0.8282902650099745,0.17170973499002548]|0        |\n",
      "|[1.0,1.0,10140.0,12889.0,7.0]  |0.0       |[0.80965520373019,0.19034479626981005]  |0        |\n",
      "|[1.0,1.0,10140.0,12892.0,-10.0]|0.0       |[0.8436152641971367,0.1563847358028633] |0        |\n",
      "+-------------------------------+----------+----------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|features                       |trueLabel|rawPrediction                           |probability                             |prediction|\n",
      "+-------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0] |0        |[1.5664227340446095,-1.5664227340446095]|[0.8272730425926873,0.1727269574073127] |0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,-3.0] |0        |[1.5828672546827551,-1.5828672546827551]|[0.8296102079456423,0.17038979205435775]|0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0]  |0        |[1.5409383037766462,-1.5409383037766462]|[0.8236010855806822,0.17639891441931776]|0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,35.0] |1        |[1.0517672098720414,-1.0517672098720414]|[0.7411141075624572,0.25888589243754273]|0.0       |\n",
      "|[1.0,1.0,10140.0,11292.0,-1.0] |0        |[1.5550091111410471,-1.5550091111410471]|[0.8256360260370816,0.17436397396291847]|0.0       |\n",
      "|[1.0,1.0,10140.0,11292.0,4.0]  |0        |[1.485127526297532,-1.485127526297532]  |[0.8153458143542055,0.18465418564579456]|0.0       |\n",
      "|[1.0,1.0,10140.0,12266.0,838.0]|1        |[-10.168331927255013,10.168331927255013]|[3.83647958813945E-5,0.9999616352041186]|1.0       |\n",
      "|[1.0,1.0,10140.0,12889.0,-2.0] |0        |[1.5735581905924647,-1.5735581905924647]|[0.8282902650099745,0.17170973499002548]|0.0       |\n",
      "|[1.0,1.0,10140.0,12889.0,7.0]  |0        |[1.4477713378741377,-1.4477713378741377]|[0.80965520373019,0.19034479626981005]  |0.0       |\n",
      "|[1.0,1.0,10140.0,12892.0,-10.0]|0        |[1.6853773163780612,-1.6853773163780612]|[0.8436152641971367,0.1563847358028633] |0.0       |\n",
      "+-------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(testingDataFinal)\n",
    "predictionFinal = prediction.select(\n",
    "    \"features\", \"prediction\", \"probability\", \"trueLabel\")\n",
    "predictionFinal.show(truncate=False, n=10)\n",
    "prediction.show(truncate=False, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 446197 , total data: 540926 , accuracy: 0.8248762307598452\n"
     ]
    }
   ],
   "source": [
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['trueLabel']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification-Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier model is trained!\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(\n",
    "    labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
    "#train our classifier\n",
    "model = classifier.fit(trainingDataFinal)\n",
    "print (\"Classifier model is trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is trained!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "model2 = RandomForestClassifier(\n",
    "    numTrees=3, maxDepth=5, seed=42, labelCol=\"label\",featuresCol=\"features\")\n",
    "model2 = model2.fit(trainingDataFinal)\n",
    "print (\"Model is trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|features                      |prediction|probability                             |trueLabel|\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|0.0       |[0.9273749301915157,0.07262506980848434]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,-3.0]|0.0       |[0.9273749301915157,0.07262506980848434]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0] |0.0       |[0.9273749301915157,0.07262506980848434]|0        |\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "correct prediction: 501376 , total data: 540926 , accuracy 0.9268846385642399\n"
     ]
    }
   ],
   "source": [
    "prediction = model2.transform(testingDataFinal)\n",
    "predictionFinal = prediction.select(\n",
    "    \"features\", \"prediction\", \"probability\", \"trueLabel\")\n",
    "predictionFinal.show(truncate=False, n=3)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['trueLabel']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", \n",
    "      totalData, \", accuracy\", correctPrediction/totalData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|prediction|trueLabel|            features|\n",
      "+----------+---------+--------------------+\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       1.0|        1|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       1.0|        1|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "+----------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "prediction.select(\"prediction\", \"trueLabel\", \"features\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0731154\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0    |\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |0    |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|0    |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|0    |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0] |0    |\n",
      "+------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv data with our defined schema\n",
    "flightDataFrame = spark.read.csv('C:/Users/hossein/Downloads/dataset/flights.csv',schema=flightSchema,header=True)\n",
    "assembler = VectorAssembler(inputCols = [\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "    \"DepDelay\"], outputCol=\"features\")\n",
    "trainingDataFinal = assembler.transform(\n",
    "    trainingData).select(col(\"features\"), col(\"Late\").alias(\"label\"))\n",
    "trainingDataFinal.show(truncate=False, n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = trainingDataFinal.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is trained!\n"
     ]
    }
   ],
   "source": [
    "model2 = DecisionTreeClassifier(seed=42, labelCol=\"label\",featuresCol=\"features\")\n",
    "model2 = model2.fit(trainingData)\n",
    "print (\"Model is trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = model2.transform(testingDataFinal)\n",
    "predictionFinal = prediction.select(\n",
    "    \"features\", \"prediction\", \"probability\", \"trueLabel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|prediction|trueLabel|            features|\n",
      "+----------+---------+--------------------+\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       1.0|        1|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       1.0|        1|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "|       0.0|        0|[1.0,1.0,10140.0,...|\n",
      "+----------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "prediction.select(\"prediction\", \"trueLabel\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0731283 \n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-boosted tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "#read csv data with our defined schema\n",
    "flightDataFrame = spark.read.csv('C:/Users/hossein/Downloads/dataset/flights.csv',schema=flightSchema,header=True)\n",
    "assembler = VectorAssembler(inputCols = [\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "    \"DepDelay\"], outputCol=\"features\")\n",
    "trainingDataFinal = assembler.transform(\n",
    "    trainingData).select(col(\"features\"), col(\"Late\").alias(\"label\"))\n",
    "trainingDataFinal.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = trainingDataFinal.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is trained!\n"
     ]
    }
   ],
   "source": [
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\",featuresCol=\"features\", maxIter=10)\n",
    "model2 = gbt.fit(trainingData)\n",
    "print (\"Model is trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Read file into dataFrame\n",
    "Description for each column data:\n",
    "\n",
    "CustomerName: name of customer\n",
    "Age: age of customer (in year)\n",
    "MaritalStatus: (1=married, 0=not married)\n",
    "IncomeRange: income per year (in USD)\n",
    "Gender: (1=female, 2=male)\n",
    "TotalChildren: number of children customer has\n",
    "ChildrenAtHome: number of children living with customer (in the same home)\n",
    "Education: (1=high school, 2=bachelor, 3=master, 4=PhD, 5=Post-doc)\n",
    "Occupation: (0=unskilled manual work until 5=professional)\n",
    "HomeOwner: (1=owning a home, 0=not owning a home)\n",
    "Cars: number of car customer has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read csv file using automatically inferred schema\n",
    "customers = spark.read.csv('C:/Users/hossein/Downloads/dataset/customers.csv', inferSchema=True, header=True)\n",
    "customers.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\n",
    "    \"Age\", \"MaritalStatus\", \"IncomeRange\", \"Gender\", \"TotalChildren\", \n",
    "    \n",
    "    \"ChildrenAtHome\", \"Education\", \"Occupation\", \"HomeOwner\", \"Cars\"], \n",
    "                            outputCol=\"features\")\n",
    "data = assembler.transform(customers).select('CustomerName', 'features')\n",
    "data.show(truncate = False, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create k-Means clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=assembler.getOutputCol(), predictionCol=\"cluster\", k=5)\n",
    "model = kmeans.fit(data)\n",
    "print (\"Model is successfully trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print centroid for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(data)#cluster given data\n",
    "prediction.groupBy(\"cluster\").count().orderBy(\"cluster\").show()#count members in each cluster\n",
    "\n",
    "prediction.select('CustomerName', 'cluster').show(10)#show several clustered data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9268716977923043\n"
     ]
    }
   ],
   "source": [
    "silhouette = evaluator.evaluate(prediction)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
